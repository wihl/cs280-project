%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-harv.tex 4 2009-10-24 08:22:58Z rishi $
%%
%%


%% WHAT WE WERE USING \documentclass[final,authoryear,11pt,times]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,authoryear,1p,times]{elsarticle}
%%\documentclass[final,authoryear,1p,times,twocolumn]{elsarticle}
%%\documentclass[final,authoryear,3p,times]{elsarticle}
%%\documentclass[final,authoryear,3p,times,twocolumn]{elsarticle}
\documentclass[final,authoryear,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}

%%\usepackage[margin=1.25in]{geometry}


\usepackage{todonotes}
\usepackage{epigraph}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{soul}
\usepackage{color}
\usepackage{cleveref}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\onehalfspacing

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon (default)
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   authoryear - selects author-year citations (default)
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%   longnamesfirst  -  makes first citation full author list
%%
%% \biboptions{longnamesfirst,comma}

% \biboptions{}

\journal{CS280r, Spring 2017 - Final Project Report, Goldstein and Wihl}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{$A \rho \mu o \nu \acute{\iota} \alpha$ (Harmonia): A System for Collaborative Music Composition}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{{\rm Mark Goldstein, David Wihl}\\ Harvard University}
\address{\normalsize\{markgoldstein,davidwihl\}@g.harvard.edu}

\begin{abstract}

Increasing productivity of music composition has many positive benefits. Listeners would appreciate music individually tailored to their emotional needs and context. Composers would be facilitated by greater and more diverse cooperation, yielding more innovative music. Therapists can use music as part of a treatment plan for autism and many other disorders. Harmonia attempts to address this myriad of needs by offering two key innovations: a SharedPlan with versioning to mediate the workflow of collaborative composition, and an algorithmic evaluation system that analyzes the proximity of a composition to a stated intention, providing guidance to both human and agent composers.

\end{abstract}

\end{frontmatter}

\section*{Introduction}
\label{sec:introduction}

%\epigraph{It is my design to render it manifest that no one point in its composition is referrible either to accident or intuition -- that the work proceeded, step by step, to its completion with the precision and rigid consequence of a mathematical problem.}{\textit{Edgar Allen Poe, The Philosophy of Composition}}

Music composition is often an individual endeavor, which is counterintuitive when music is mostly performed, improvised, and experienced in a group. Part of this is due to the singular nature of creative expression, but a large part is also due to a dearth of viable tools to enable collaborative efforts between composers. Most modern composition is facilitated through the use of digital audio workstation (DAW) tools, yet composers do not generally have access to tools designed for collaboration, version control, and annotation that are available to software engineering teams.

There are many additional issues that surround collaboration over structured, shared objects such as a music composition. Each composer has a unique artistic vision, and a composer working on one section of a piece may ruin the plans of another working on a later section. It is crucial in such settings that a group specify an artistic goal clearly and communicate their intentions for material as it is added. However, it is also valuable that new ideas can surface as a consequence of the collaborative process, leading to material that could not be created by any one artist. How can composers stay connected with an original goal and also make room for spontaneity?

Collaborative Ideation (CI) investigates the effect of various idea-sharing tactics on the creativity, productivity and scope of individual idea-generating participants. Such techniques may include showing each participant in a pool of brainstormers the collection of all participants' brainstormed ideas. In CI that focuses on the creation of shared objects such as a composition or story by several people, certain interfaces for collaboration, automated idea-sharing mechanisms, and per-participant system feedback may help facilitate group creativity and productivity.

SharedPlans provide a framework to establish and maintain context and metadata about group artistic work. This framework provides a formalized notion of common beliefs, and defines a computational approach to ensure that the collaborative process stays within reasonably agreed parameters, preserving original intention.

We propose Harmonia, a system that addresses collaborative music composition inspired by tools used for software teamwork, collaborative ideation, and musical analysis. The system defines a set of editing procedures and an interface for communication and collaboration over a shared musical artifact.

In \S \ref{sec:related} we examine previous work related to shared intentionality, collaboration and group ideation, intelligent music systems, and modeling the structure of music. In \S \ref{sec:design}, we describe the details of the overall usage workflow, as well as the individual components of our system. A proposed interface for users and composers is described in \S \ref{sec:UI}. The details of the automated analysis of a work-in-progress is described in detail in \S \ref{sec:analysis}.  We present four use cases for our system in \S \ref{sec:usecases}, where collaborating composers create music ranging in purpose from casual listening to clinical therapy. In \S \ref{sec:discuss}, we discuss limitations identified in the system, and a few initial plans for system validation. We discuss potential future work in \S \ref{sec:future}.

\section{Related Work}
\label{sec:related}

Our work builds on several areas of research related to multi-agent systems, creative cognition, and music. First, we discuss related work regarding shared intentionality in multi-agent settings. Then we discuss collaborative ideation, both in general and specifically in music. Next we discuss intelligent music systems that facilitate human composition and improvisation, and related work in Music Information Retrieval (MIR). Finally, we describe previous work that applies information theory to the analysis of musical structure.

\subsection{Shared Plans}

SharedPlans \citep{grosz1996collaborative, hunsberger1998making} define a robust framework for multi-agent, multi-level collaboration. A SharedPlan consists of a set of intentions, mutual beliefs, constraints, and hierarchies of actions and plans. All of these elements are applicable in this context, although only a subset of the general definition is needed for music composition purposes. 

A SharedPlan is the basic unit of work for a collaborative group in our system. It serves as the container for the initial definition of the intent of the work, the musical score representing the work-in-progress, the revision history, the subplans used to communicate between composers, and the final approval given by the user who defined the SharedPlan initially.

Intentions are expressed by a minimum of two criteria: genre and mood. Mutual beliefs are expressed implicitly in the trust of the automated analysis system that guides the collaborative process (See \S \ref{sec:analysis}) and explicitly through subplans. There is one complex action that characterizes the full SharedPlan in this scenario: production of a score that meets the intention. Each composer may divide the complex action into a series of sub-actions (See \S \ref{subsec:editActions}~\nameref{subsec:editActions}). If a composer is unable to complete all actions necessary to achieve the intention, a subplan is created stating the Intention-To for a future composer.  This is the primary means by which composers communicate with each other. When a subgroup of composers is working on a subplan, they share a restricted set of mutual beliefs about the intention of the subplan. We do not define a means of using \textit{recipes} in this context.

We use the term composer in the singular form, however a single composer may work as part of several collaborating humans to produce an edit. Any such informal communication is beyond the scope of our proposal. After collaborating, a single human would execute edits on composition toward the intended goal. For example, a composer may improvise with colleagues to explore several alternatives of a subplan. Once a consensus is achieved that the section meets artistic and SharedPlan goals, one composer commits an edit.

\subsection{Collaborative Ideation}
% Collaborative Ideation In General
Collaborative Ideation (CI) improves the productivity of individuals and groups in generating ideas through collaboration. People intend to create related objects (e.g. brainstorm solutions to social problems) and seek either feedback or examples of others' work to enhance their individual process. Collaboration is centered in a shared workspace, physical or virtual, that allows for communication and sharing of ideas. The ideas produced may be for individual use, or ideators may work on shared artifacts such as an essay or piece of art. The dynamics of collaboration may be real-time or not, though increasingly, today's settings are real-time and virtual\footnote{\texttt{https://www.openideo.com/}}\footnote{\texttt{http://www.ideastorm.com/}}\footnote{\texttt{https://www.quirky.com/}} \citep{openinnovation}. A simple CI setting is one where each ideator brainstorms solutions to a problem common to all participants, and each participant can see all other's ideas. The design of intelligent computer systems today aims to facilitate these activities to allow for increased creativity and productivity. 

%IdeaHound
Only a small subset of an idea pool may be relevant and inspiring to a single ideator, and it is overwhelming for each ideator to view all participant's ideas \citep{siangliulue2015toward}. Ideahound \citep{siangliulue2016ideahound} prompts each user to interact with a personal ``whiteboard" where they can cluster their ideas and separate them by semantic distance. The system compiles the whiteboards into a global map, allowing each ideator to view their work in the context of the entire solution space. IdeaHound recommends diverse suggestions to each ideator, eliminating the cognitive load of idea search. 

When exposed to the work of others, composers may create things they would not make on their own.  In our work, In Harmonia, CI is made explicit when a composer addresses a subplan instantiated by another (See \S \ref{subsec:interComposer}). Objects are structured, rather than unordered collections of ideas, and ideators need to build over each other's ideas rather than only seek inspiration from examples by others. These details present new challenges.

%Soundcloud and Blend
CI has surfaced in the space of online blogs and services designed for sharing visual art and music. Ideas range from small, unfinished efforts seeking direction to finished pieces seeking critique. Artists improve upon their ideas using large-scale peer feedback. SoundCloud is a hybrid music streaming service and CI platform. Though music is often presented in finished form, people also post incomplete projects. Artists sometimes share ``stems" to their music, which are individual sound files that feature isolated instrumental tracks, with the intention that others seeking inspiration will remix their pieces into new work. 

The Blend platform\footnote{\texttt{https://blend.io}} makes the sharing of source files explicit. By default, artists share their works-in-progress in the format of music production software source files. This setting encourages building on each other's ideas. However, people mostly work toward an ultimately individual direction. What changes when several ideators intend to create a shared piece? In our work, a composition is associated with a specified intent through the duration of its existence. It is up to the composers and the system to keep a piece of music close to its SharedPlan.

\subsection{Computer Facilitated Composition and Improvisation}

% Music Information Retrieval
Intelligent Music Systems ranging from improvising agents to recommendation systems have a common requirement: to ``understand" music at multiple levels, including low-level acoustic signal, mid-level theoretical constructs such as harmony and rhythm, and high-level level concepts such as mood and genre. For example, music recommendation systems such as Spotify\footnote{\texttt{https://www.spotify.com}} seek to analyze music and extract a measure of relevance for a function such as ``study music." These issues constitute the research area of Music Information Retrieval (MIR).

% ChordRipple
Human composers are assisted, not replaced, by creative agents. A composer with good ``seed" ideas may receive recommendations from an agent for variations or re-orderings of ideas that make them more conveying. Such system knowledge often comes from large-scale corpus analysis that mines patterns from collections of music. ChordRipple \citep{huang2016chordripple} takes as input a progression of chords from a composer, and suggests substitutions of intermediate chords that preserve the original semantics of the input while serving to replace conventional choices with more interesting ones. If the composer agrees to make one of the recommended changes, the system assists the composer in interpolating between original and substituted material, resulting in a mix of human and system generated music. Harmonia makes suggestions to composers, but it focuses on structural changes to support collaboration rather than substitutions of material.

% Magenta and AI Duet, Connection to Therapy Use Case
While the current system seeks to assist human composers to enrich and organize collaborative work, our design permits automated agent composers to fully participate without requiring human intervention. Google Brain's Magenta project explores the limits of machine creativity in art and music\footnote{\texttt{https://magenta.tensorflow.org/welcome-to-magenta}}. Magenta is an integrated environment of software tools and music-related datasets. Built on Magenta, AI Duet\footnote{\texttt{https://aiexperiments.withgoogle.com/ai-duet}} reacts to human improvised gestures. Improvisation is an important part of composition. When a single composer is brainstorming, it may be beneficial to improvise on ideas with an automated agent, much like two musicians iteratively vary and refine their ideas in live rehearsals. A listener may need music at a certain tempo and with a simple beat. In settings where a piece is defined by a specific-enough set of guidelines (See \S \ref{subsec:therapy1}), powerful information retrieval systems may make effective machine composition agents possible. Human composers may be placed at later steps of collaboration to ensure that the piece meets requirements in humanly perceptible ways.

\subsection{Information Theory and Music Analysis}

% Abdallah, Predictive Information Rate
Harmonia relies on the ability to analyze musical structure to support automated feedback for collaborating composers. Recent analytical approaches have represented musical form in the context of listener perception, modeling the attention dynamics of the listener as they experience the toggle of surprise and redundancy. The Information Dynamics Approach \citep{abdallah2012cognitive} uses \textit{predictive information rate}, an entropy-based measurement of how a listener's distribution over future musical events is continually revised as new information is presented.  Our work assumes that structure can be effectively summarized by this criteria. We assume that pieces from a particular combination of genres and moods are defined by characteristic balances of surprise and redundancy over time, with peaks of information content in genre-specific locations. Harmonia uses the Information Dynamics Approach to compare a work-in-progress to the characteristic curves for the genre and mood specified by the work's SharedPlan.

\section{System Design}
\label{sec:design}

The system design addresses two aspects of collaborative composition. The first includes the composition interface, the representation of music, the editing capabilities of composers, and the version control system used to track musical revisions. These all focus on issues of communication and coordination in collaboration over a shared object. The second aspect of design is the automated music analysis used for giving feedback and suggestions to composers. This facilitates the collaborative composition process by helping composers assess the effect of their edits on the movement of the work-in-progress toward a goal. We discuss how these two system components combine to help composers by providing an interface that makes revisions convenient, modular and amenable to collaboration.

\subsection{Workflow Overview}

\begin{figure}
	\includegraphics[scale=0.35]{workflow.pdf}
	\caption{Overall workflow: a non-musical user or a composer define a SharedPlan. Composer 1 (\textcolor{red}{red}) starts
	iterating and commits a work-in-progress. Composer 2 (\textcolor{blue}{blue}) continues iterating until the original user is
	satisfied.}
	\label{fig:workflow}
\end{figure}

The generic use case that we consider is that a listener (musician or not) requests a new piece of music of a specified genre and mood. A new, blank score is associated with a SharedPlan that stores the genre and mood. Additional descriptive keywords may be included in the form of tags, like those on YouTube or Soundcloud\footnote{\texttt{https://on.soundcloud.com/creator-guide/tracks}} (e.g. \texttt{$\#$USA}, \texttt{$\#$lo-fi}, \texttt{$\#$chill}, \texttt{$\#$electronic}, \texttt{$\#120$BPM}). These tags power an automated comparison between work-in-progress and goal, and helps composers pick edits that may bring a composition closer to a sound characterized by the goal keywords. We assume that an information retrieval system for querying MIDI music scores by keyword exists.

Composers iteratively add and edit material while receiving system feedback, helping composers choose which edits are most relevant to the SharedPlan. The system also suggests edits, such as to repeat, delete, or the switch the order of blocks of material. 

Composers can follow suggestions or make their own decisions. During this process, composers create subplans that describe the intentions of local edits, such that others can pick up on their material and preserve these intentions. Composers collaborate to bring the composition to a state of completion such that it matches the originally specified goals and the SharedPlan originator is satisfied. The finished product is a MIDI score.

% Do Not Delete

%The final product of collaboration is a MIDI file that represents the pitches, rhythms, and dynamics of musical events, as well as a collection of metadata that concisely states additional information, such as tempo and instrumentation. 

% The piece can be produced as an electronic piece of music by importing the MIDI into any music production software, specifying which MIDI track should be played by which sound or synthetic instrument, and exporting a sound file. If desired, composers may take additional actions on that finished work that are external to the system, such as produce the piece of music with live musicians or extended software instruments, but this is beyond our scope, which is mainly to facilitate collaboration during the composition process.

\subsection{Version Control}

Version control has become an essential part of team-based software development. However, the tools for managing revisions of other creative work, including composition, are nonexistent or limited in comparison. There are two commercial version control systems for music: Blend and Splice\footnote{\texttt{https://splice.com/}}. \texttt{git} \citep{torvalds2010git} has been referenced elsewhere \citep{oberholtzer2015computational} as a viable means for music score version control, especially in the MIDI format. Software-based version control systems are excellent for basic operations such as branching, committing, and viewing history, but they lack several primitives necessary for integration in our proposed workflow. We propose the following additional commands:

\noindent \textit{For users and composers}:
\begin{description}
    \item [EditPlan] - create, revise or delete a SharedPlan that includes intentionality and other metadata.
    \item [Approve] - the SharedPlan creator denotes a SharedPlan as satisfactorily executed
    \item [Reject] - the SharedPlan creator denotes a SharedPlan as unsatisfactorily executed
\end{description}

Note that Approval or Rejection by a user of a composition may be implicit based on their listening behavior (See \S \ref{subsec:userInterface}~\nameref{subsec:userInterface}). 


\noindent \textit{For composers only}:
\begin{description}
    \item [Evaluate] - prior to committing a revision, perform an analysis of the current score against intentionality.
    \item [EditSubplan] - create, revise or delete a subplan. A composer defines a subplan to fragment the SharedPlan into a tractable unit of work or express the call for assistance for another composer to provide input.
    \item [Release] - after a commit, mark a work as ready for review by the creator of the subplan. This closes the subplan. If a plan is rejected, it may be re-opened.
\end{description}

Versioning is particularly relevant in the music context, as intellectual property and originality is often the source of extensive lawsuits. By maintaining version control, an examination of the history would assist in the identification of derivative works and originality of authorship.

\subsection{Communication through SharedPlan and Subplans}
\label{subsec:interComposer}
 
When a composition task is created, it exists as a SharedPlan that contains an intent specification, as well as a \texttt{NULL} score. In addition to describing global goals, the SharedPlan mediates any inter-composer communication that takes place aside from the edits to the composition. A composer may wish to express the intention of a particular edit to justify its presence or to elicit specific future directions. For example, a composer may add a block of music containing an exposed melody and communicate that another composer should add a harmonic accompaniment. This is done by issuing a subplan. Another composer who acts on a  subplan \textit{releases} it when they commit their work.

It is ideal for communication to be concise and structured. It may be detrimental to collaborative work for one composer to expect others to read long, unstructured goal descriptions. It may require too high a cognitive load for others to carefully read and understand, and the intending composer may not have their ideas respected. The system does not yet feature a structured language for describing intent in subplans, but recommends that composers be concise. A structured language may help support automated agent collaborators in the future. This is discussed further in the Therapy use case (See \S \ref{subsec:therapy1}). 

Intention sharing may help distinguish between two kinds of collaboration issues. One scenario is where a composer agrees with another composer's intention about an edit, but disagrees with the implementation. This may lead the second composer to edit the first's material with care to preserve the original sound, or to replace it with something of similar effect. The other scenario is where one fundamentally disagrees with the artistic intention of the other. An effective SharedPlan communication system may disambiguate these cases.


\begin{table}
    \begin{tabular}{|l|l|}
    	\hline
    	\textbf{Action} & \textbf{Description} \\
    	\hline
    	\textit{Insert,Delete,Replace} & Add, remove or replace a block \\
    	\textit{Move} & Rearrange order of a block \\
    	\textit{Split, Merge} & Divide or combine a block \\
    	\hline
    \end{tabular}
	\caption{Summary of Atomic Edit Actions on Score}
\end{table}

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{midi.pdf}
	\caption{Music represented by MIDI (top) and MusicXML (bottom)}
	\label{fig:midi}
\end{figure}



\subsection{MIDI and Edit Actions}
\label{subsec:editActions}

Our system represents music in MIDI format. MIDI is a protocol for communicating information about the pitch, duration, and dynamics of individual notes. A musical work is described by specifying the vertical arrangement of notes as chords and their horizontal arrangement over time. There is a General MIDI Sound bank that maps integers to electronic instrument sounds such as \textit{grand piano} and \textit{ocarina}. Most software packages for music notation and production build user interfaces over basic MIDI editors. Composers can segment MIDI files into blocks corresponding to phrases, themes, or sections. Segmentation may represent intentions about musical form, for example a segmentation into an \textit{exposition} and a \textit{development}.

During an interaction with the system, a composer is able to change a composition by editing the MIDI in several low-level or high-level ways. Composers can add a new block, edit or remove an existing block, swap the order of two blocks, merge two blocks, or split a block into two (See Table 1).

%At each iteration, the system suggests an option that may bring the work-in-progress closer to what is specified by the SharedPlan (See \S \ref{subsec:suggestedit}).

\subsection{Design Failure Modes}

There are multiple potential failure circumstances of the proposed design:

\begin{description}
\item[Failure to compose] No composer may elect to work on a given SharedPlan.
\item[Failure to satisfy] The creator of the original SharedPlan may never approve or reject the produced work, leaving it in an undefined state.
\item[Subplan not released]A subplan may be defined but no composer ever completes the subplan, leaving the work in an incomplete state.
\item[Evaluation not converging] The evaluation process may continue to offer suggestions that are rejected by the composer. Composers may abandon using evaluation. \item[Failure to evaluate] there may not be sufficient training data for a given genre / mood combination so the evaluation may fail to provide sufficiently useful recommendations.
\end{description}

Failures result in a specific SharedPlan not advancing, but do not affect other SharedPlans in the system. A SharedPlan requester may still accept a piece under some of these conditions.

\section{User Interface}
\label{sec:UI}

There are three aspects to the user interface: for SharedPlan requesters, for composers in workflow, and for composers actively editing the musical score.

\subsection{SharedPlan Workflow User Interface}
\label{subsec:userInterface}

There are three possible commands in this context: \textbf{EditPlan} (including create/edit/delete), \textbf{Approve, Reject}. Since the user may not have any musical background, this user interface must be accessible to as large a population as possible, be a simple-looking and reactive web-based interface, and work equally well on mobile devices and desktops. Music applications such as Spotify may wish to integrate this functionality, so a web-based API must also be accessible.

For an unsophisticated user, \textbf{Approve} and \textbf{Reject} may be considered implicit. If the user listens to the composition past a certain point, such as 80\% of the total length of the work, it may be deemed implicitly approved. Similarly, if the user advances to another song within the first 20\% of the work, the composition is implicitly rejected. Implicit rejection does not provide any causal reason for the rejection, so trial and error analogous to reinforcement learning must be used to discover why a work was rejected. Other user actions (such as stopping all music in the middle of play) do not provide enough signal to determine implicit actions, so the work's acceptance status remains unknown. Explicit approval / rejection is unlikely to be used in practice by users with short attention spans. 
%The system will learn user preferences more accurately by empirical observation.

\subsection{Subplan Workflow User Interface}

The market reality is that most modern composers use an existing Digital Audio Workstation (DAW) such as Ableton or LogicPro. Rather than replacing the composer's primary user interface, integration should be performed within the menu system of the most common DAWs. There are only two commands required in this context: \textbf{EditSubplan} (including create/edit/delete) and \textbf{Release}. 

\subsection{Composer Edit / Evaluation User Interface}

This is the most complex and demanding user interface as it requires the highest degree of interactivity. Rather than re-implementing the multitude of editing features in a mature DAW, integration to existing DAWs is again preferable over creating a \textit{de novo} user interface. The most novel aspect would be the integration of the evaluation to the editing task. Ideally the evaluation result would decorate and annotate the score; enabling the composer to address results of the evaluation while remaining in a known editing context, analogous to seeing and resolving comments in Microsoft Word or a Google Doc. The design of this feature is not yet complete.

\section{Automated Analysis of Musical Structure}
\label{sec:analysis}

Automated analysis lets composers know how close their work-in-progress is to its goal. Additionally, it suggests structural edits that could further improve the piece. Crucial to our system, our computational approach models the structure of a piece of music in relation to the expected trajectory of surprise and redundancy that a listener experiences. Our work follows the Information Dynamics Approach. We first discuss the nature of the musical analysis used in our system, and then discuss how this method supports goal checking and edit suggestion.

\subsection{Entropy of Musical Events and Divergence}
 
Let $X$ be a discrete random variable that takes on values from the set $\mathcal{X}$. For example, $X$ may represent the next chord that a listener hears in a piece of music. The event $X=x$ indicates that the listener heard $X$ take on a specific value $x$. Let $p_X(x) = p(x)$ denote the probability that $X$ will take on value $x$, \textit{before} the listener hears the event, as estimated by a distribution that the listener brings with them from prior musical experiences, as well as from what they have heard in the piece so far. $-\log\ p(x)$ then corresponds to the \textit{surprise} of the event, because the more the listener expects the event, the lower the surprise, where the log is taken for convenience. Since $X$ represents the event that the listener is about to hear, we can represent the expected surprise of $X$ averaged over all possible values it may take on as:
 
 $$ H(X) = - \sum_{x \in \mathcal{X}} p(x) \log p(x)$$
 
\noindent which corresponds to the Entropy of $X$, $H(X)$. Intuitively, this means that the listener does not know what the next event will be (e.g. which chord will be played next), but from context they expect a certain level of surprise from the next event in general. The current state of listening is maintained as a distribution over future events.

Since we choose to represent musical structure in terms of the surprise dynamics of the listener, it is necessary to describe the way in which a listener's distribution over future events changes as they hear each new event. After hearing the event $X=A\flat^{\Delta7}$, how does the distribution differ from how it was prior? The Kullback-Leibler Divergence from one distribution of another captures this notion of distance between distributions. Avoiding subscripting $X$ with timesteps, let  $X'$ be the revised distribution over the next event \textit{after} hearing $X=x$ in the context of the existing distribution.

$$ D_{KL}(X^{\prime}\textrm{ }|| \textrm{ } X) =  \sum_{x \in \mathcal{X}} p_{X^{\prime}}(x) log\frac{p_{X^{\prime}}(x) }{p_{X}(x)}$$

\noindent This is read as ``the divergence from $X^{\prime}$ of $X$", and is the average over the log ratio of point-wise probabilities between the two distributions, weighted by $p_{X^{\prime}}(x)$. For an accessible yet informative discussion of the significance of entropy as a measure of information and KL Divergence\footnote{Another interpretation of entropy is the average number of bits required to send a message from a distribution $p$ under an optimal variable-length coding scheme. The KL Divergence of $q$ from $p$ is the increase in the average bits per message when one communicates items from $p$ using a code optimized for $q$. This is the difference between the cross entropy $H(p,q)$ and entropy $H(p)$.}, see Christopher Olah's post on Visual Information Theory\footnote{\texttt{http://colah.github.io/posts/2015-09-Visual-Information}}. 


Let $D_{KL}(X^{\prime}\textrm{ }|| \textrm{ } X)$ be called the instantaneous \textit{predictive information} of the event $X=x$ as the listener hears it. When a surprising event occurs and causes the listener to drastically revise their distribution (i.e. this same event will be less surprising in the future), this corresponds to high predicative information. On the other hand, if thirty strikes of the same chord have just happened, hearing a thirty-first articulation does not communicate much predictive information. Our system measures the predicative information \textit{rate} (PIR) over the duration of the piece (or work-in-progress), and uses the trajectory of this rate to summarize the structure of a piece of music as it is expected to be perceived by the listener. In the running example, entropy and divergence are discussed with respect to chord sequences heard by the listener and the their expectation over next chords in context. In even a simple piece of music, the listener tracks multiple interacting parameters: evolving harmony, rhythm, timbre, and more (See \S \ref{sec:future}~\nameref{sec:future}). Our system calculates predictive information by averaging the quantity across trigram chords at each time step.

\subsection{Current Design: Analyze, Suggest, and Edit}
\label{subsec:suggestedit}

Harmonia uses PIR to calculate the proximity of a work-in-progress to the SharedPlan goal. Using PIR, the system gives feedback to a composer with respect to the composer's editing decisions, and provides suggestions that may bring a piece closer to the goal.  We calculate the PIR for the most popular $\boldsymbol{\beta}\%$ of pieces matching SharedPlan keywords (where $\boldsymbol{\beta}$  is a parameter to be specified) and create a ``characteristic curve" that represents the typical structure of music fitting the criteria.

Comparing the characteristic curve with the PIR curve of the work-in-progress, our system can estimate the distance from the musical goal specified in the SharedPlan. Let the difference be denoted as $\Delta$, where small $\Delta$ indicates that two pieces of music are similar in structure with respect to surprise and redundancy over time. Even without edit suggestions made by the system, a composer may simply see whether their latest edit brings the piece of music closer to (lower $\Delta$) or further from (higher $\Delta$) the SharedPlan. Composers may prefer to go with edits that decrease $\Delta$, or may choose to stick with their edit even if $\Delta$ increases. Reasons for going with a ``worsening" action include laying down material that further edits will re-contextualize, whether by the same composer or by others. In this case, it is important to issue a new subplan.

As specified in \S \ref{subsec:editActions}, composers may edit a composition in several defined ways. Excluding options that require advanced machine composition, the system may give actionable edit suggestions according based on PIR scores. It may recommend repeating or deleting an existing block or swapping any pair of existing blocks. For any work-in-progress of reasonable length, there is a tractably enumerable set of such choices. The system can just try each choice of deleting, repeating, and swapping, and suggest the choice that minimizes $\Delta$ to the user.

%It is this analysis and suggestion loop, along with interaction with the SharedPlan metadata, that characterize the main experience for an individual composer. This experience is further enriched by the fact that each time the composer enters the feedback loop, the piece and aspects of SharedPlan may have changed by other composers.

\section{Use Cases}
\label{sec:usecases}

\subsection{Use Case 1: Individual User, Individual Composer}

A listener, who may be a non-musician, would like a new piece of music for a very specific function such as study or exercise music \citep{karageorghis2012music}. We consider the case that the listener specifies a new project defined by a mood and genre. In this simple case, we consider a single composer who iterates over the piece with assistance from our system until the requester is satisfied.

\subsection{Use Case 2: Multiple Composers}

\begin{figure}
	\includegraphics[scale=0.35]{multicomposer.pdf}
	\caption{Use Case 2: Multiple composers define a SharedPlan. Composer 1 (\textcolor{red}{red}) starts
	iterating, commits a work-in-progress and defines a subplan. Composer 2 (\textcolor{blue}{blue}) retrieves the subplan and continues iterating until the original composers are
	satisfied. Composer 2 maintains a separate branch for personal investigation.}
	\label{fig:multicomposer}
\end{figure}

Consider multiple composers who create a SharedPlan together, and then collaboratively create the specified piece. During the process, a composer may wish to take the piece into a new direction that doesn't correspond to the SharedPlan. They are able to create a new branch of revision history to experiment on before attempting to re-integrate material with the original group or creating a new SharedPlan with the material. On the main branch, composers issue subplans to give someone else an opportunity to complete their idea when they are stuck. Even an insufficient attempt by another composer to release the subplan may inspire the issuer. See \S \ref{subsec:eval} for a discussion of composer-system interaction success criteria.

\subsection{Use Case 3: Therapist with Agent \& Human Composers}
\label{subsec:therapy1}

\begin{figure}
	\includegraphics[scale=0.35]{clinical.pdf}
	\caption{Use Case 3: A clinician defines a SharedPlan with more complex metadata for therapeutic use. An agent performs the initial composition which consists of the bulk of the work.	A human composer reviews the work and makes minor adjustments. The clinician approves the work and provides the result to the patient for treatment.}
	\label{fig:clinical}
\end{figure}

Consider a music therapist who treats their patients using newly composed music, specific to a given patient's needs. Music therapy is used in many contexts including children and adults with mood disorders (e.g. depression and PTSD), developmental disorders (e.g. autism and ADHD), and neurological conditions (e.g. dementia) \citep{hole2015music}. This SharedPlan may have a more highly refined specification than music for casual listening, such as a specific tempo or special therapeutic timbres (sound qualities). 

Due to high volume of personal treatment plans, the initial SharedPlan is worked on by an agent that performs the bulk of the composition. Consider an agent in the reinforcement learning setting, exploring the space of musical edits while in a feedback loop with the automated evaluation system. The agent's composition is then reviewed by a human who may make only small modifications to make the music more warm or less mechanical. The clinician makes the final approval and then provides the music to the patient for treatment.

\subsection{Use Case 4: Pseudo Music Therapist}

For those who do not have access to a music therapist, Harmonia can be used as a platform to assist patients in achieving certain goals and outcomes, such as creating a song that will express sadness or grief, or to express acceptance and hope for the future \citep{dalton2006grief}. The recommendations generated by the evaluation process can assist a non-musically trained composer to create reasonably sounding music for emotional expression. The act of creative composition is itself therapeutic employing various regions in the brain, including the region responsible for autobiographical memory in the case of improvisation. \citep{hilliard2001effects,bensimon2008drumming,limb2008neural,carr2012group} 

\section{Discussion}
\label{sec:discuss}

\subsection{Validation Methodology}
\label{subsec:eval}
 
We intend to validate the system with several studies on small groups of composers that collaborate over completion of a musical goal. We also intend to validate for non-musician requesters seeks for their musical desires to be addressed. Therapy use cases should be included in studies once these two less-complex cases are assessed. Testing will include the validation of several system aspects:

\begin{enumerate}

\item Do composers understand and benefit from system feedback? If the composer proposes edits $A$ and $B$, and the system scores $B$ higher than $A$, which does the composer pick? What percentage of preferred edits do composers stick with? For system-suggested edits, what percentage of suggestions do composers accept?

\item Do composers feel that communication through subplans is efficient for collaborating on ideas requiring the work of several composers?

\item Do composers and requesters feel as if finished pieces respect the SharedPlan? 

\end{enumerate}

To address point 1, we can compare composer experience using a) our revision and metadata system without automated analysis, b) system with score feedback for composer-specified edits, but no system-suggested edits, c) system with score feedback to composer-specified edits and system suggestions. To address point 2, we should explore other methods of inter-composer communication in place of the subplan issue/release procedure. Two approaches are a) traditional email to complement version-controlled music with no subplan options b) additional text documents to describe the equivalent of subplans but external to the SharedPlan and in longer form, committed along with the SharedPlan and musical score. Point 3 will come from discussion with the requesters and composers.

The percentage of edit suggestions that each composer accepts does not only describe the success of the system's suggestions, but also the personality of each composer: are they generally open to suggestions or do they tend to stick with their own ideas? This will help to not blame the system for poor performance when dealing with a composer that tends not to take suggestions in general. Aspects of a composer's collaboration profile may also be collected orthogonally to system usage through simple collaboration games (to be designed).

Several obstacles remain to setup the studies. This involves implementation of the system, including information retrieval aspects.  Compositions may take some time to work on, so these studies may not be very quick. We may design smaller-scale composition games for these purposes.

\subsection{Enhancing or Stifling Creativity}

The question arises whether algorithmic evaluation of a composition is a potential means of stifling music creativity. Is it possible that a corpus based evaluation will nudge composers into making music that resembles existing work rather free creative expression? Firstly, the evaluations and suggestions are purely advisory and can be ignored by composer. Secondly, the entropy-based evaluation does not comment on material as much as organization of material. The evaluation could be further enhanced with semantic understanding of musical material to show the composer that the work-in-progress resembles certain existing works too closely, avoiding potential future intellectual property infringement. No composer can possibly know all existing work in the corpus. A composer can now have an independent measure of the originality of the new work, which will encourage greater creativity.

\subsection{Limitations}
\label{subsec:limitations}

There are several limitations in this proposal that are inherent in the design. This is purposeful, as addressing these limitations may result in undo expansion of scope or problem tractability. Nevertheless, it is important to be explicit about known limitations:

\textit{No explicit support for improvisation.} The collaborative design is inspired by software engineering teamwork, which is a solitary activity. Unlike software, in music very fruitful results may occur during real-time improvisation by several musicians. The design does not preclude improvisation but does not account for it either. The option for two composers to branch an improvisation over existing material is not captured.

\textit{Information Theory.} The system calculates PIR over single musical parameters, such as chord progressions or rhythms, but composers create many complex relationships across parameters not captured by averaging. \cite {cohen1962information} demanded 1) a theory of interaction for multiple streams of musical information, for example, to explain how rhythmic information may affect likelihood of harmonic events 2) a theory for multiple information levels coming from a single stream. Rhythms constitute a local time feel but also accelerate a piece toward new sections. These are still complications in information today. The first point requires a generalization of mutual information to multiple random variables, which has been met with confusion through several coexisting approaches \citep{van2011two}. The second point has not yet been successfully modeled  \citep{widmer2016getting}.

\textit{Model may not generalize.} While we are confident that SharedPlans, revision control, and algorithmic evaluation are applicable to music composition, it is uncertain that the overall framework can be generalized to other domains. Version control for visual artwork seems promising, but simple text-based revision may not support editing capabilities for sufficently rich representations of visual art in an intuitive or useful way.  Algorithmic evaluation of intent may be more difficult to design for a painting. Musical form, as treated by the system, is modeled at an information-general level also applicable to a story, movie, or play, but this model is limited to sequential forms. The static experience of a painting is less related and may require a different set of domain-specific criteria for capturing intent. To this end, generalizable techniques for automated analysis that bypass the necessity to encode domain-specific information are desirable, and may be approached with today's deep learning techniques (See \S \ref{sec:future}~\nameref{sec:future}).

\textit{Representation is MIDI, not recorded acoustic signal. Vocals are not supported.} MIDI is far easier to generate and maintain under version control, but has two limitations. The first is that many interesting sounds, such as vocals and other samples of recorded sound, cannot be expressed in the current design. MIDI placeholders may be placed, such that someone can substitute recordings  post-collaboration, but collaboration over these sounds themselves is not supported. Given this setting of \textit{written}, rather than \textit{performed} music, the second limitation is that MIDI encodes only a subset of written musical expression. MusicXML is another text-based format that is more flexible than MIDI, while still being amenable to revision control (See Figure 2). It allows musicians to visually communicate performance techniques (e.g. pizzicato, slur, sul ponticello) but would complicate SharedPlan communication protocol and automated analysis.

\textit{Corpus based shared beliefs may not be robust.} One of the key assumptions is that genre / mood permutations generate a reasonable representation of the intent of the music. This representation is corpus-based. This assumption presumes that genre and mood classification solutions exist and that the representation is robust enough for the evaluation function to provide useful guidance. Since the system has not yet been built, certainty of this assumption remains a source of further investigation.

\section {Future Work}
\label{sec:future}

Harmonia is intended to serve as a base platform for multiple aspects of collaborative composition, and is modular enough such that individual functionality within the framework may be replaced with newer techniques. The area of creative agents is undergoing rapid evolution. Providing a framework to ensure shared intentionality, artistic consistency, and effective collaboration among humans should provide further acceleration. Multi-agent online improvisation such as AI Duet is another ripe area for exploration both for artistic purposes as well as for `call and response' treatment of autism and other disorders.

The current automated analysis system is based on hand-crafted entropic measures specific to music information. Given the rapid advance of Recurrent Neural Nets in natural language understanding and other sequential data streams, further investigation is warranted for applying deep learning to musical evaluation. In general, a deep learning approach to \textit{analysis} of music \citep{deepgenre} \citep{deepfeature} is a huge and exciting obstacle in the training of automated, musically interactive agents, which could have roles in the system. If Harmonia were used by a large set of users, the resulting listening dataset would provide important training data for applying Reinforcement Learning and other methods to further enhance agent composition. A larger dataset would also be of significant benefit to music therapists who could share empirical data on effective treatments tailored to specific demographic and psychographic profiles.

There are significant limits when using MIDI for musical expression. Incorporating more complex musical encoding formats such as recorded audio, or at least MusicXML, would allow for greater range of expression but would complicate analysis. Adding vocals is an interesting challenge. Given recent developments in speech generation, is it possible to extend the idea to vocals?\footnote{\texttt{https://lyrebird.ai/}} Can a neural net be programmed to `sing' like Alicia Keys?  Commercial applications of Harmonia include the aforementioned intellectual property similarity evaluation. The analysis module may also incorporate commercial goals such as potential popularity \citep{phampredicting}.

\section*{Conclusion}
  
Our proposal contributes two novel areas of collaborative music composition. First, we present a workflow incorporating SharedPlans with explicit intentionality, subplans, Collaborative Ideation, revision control and automated agents. Second, we present algorithmic evaluation of a composition against intention, to ensure that human and agent composers cooperate in reaching the shared objective. These contributions extend the state of the art in collaborative composition frameworks, beyond the ideation of SoundCloud and the revision control without intentionality of Blend. We see this initial proposal as a start of several interesting areas to explore leading to improved creativity, diversity and applicability of music composition as a tool for composers, music therapists and listener pleasure.
 
\section*{Acknowledgements}

We are honored to be part of Professor Grosz's final class. We deeply appreciate the guidance that she and Peter Krafft have provided all semester. We wish them all the best in their future endeavors and SharedPlans.

Special thanks to \href{https://www.psychologytoday.com/experts/david-m-greenberg-phd}{Dr. David Greenberg}, who provided important insights and suggestions regarding the application of Harmonia for music therapy, to composers Daniel Pencer and Ariel Herbert-Voss, whose discussions about the proposed user interface of our system were highly valuable, and to Pao Siangliulue, for the discussion about composer communication.


%%%%%%%%% OPEN ITEMS %%%%%%%%%%%%%%%%%

%------------------ MARK TODOs --------------------------------

%1) Reduce Related Work

%2) See if possible to remove the extra space after the section numbers in the refs

%3) related to collaborative ideation somewhere. Maybe in MIDI and Edit Actions section.

% NOTE: Replaced the class-provided elsarticle-num-names with elsarticle-num-names-alpha
%       to enable sorting of references.
% See https://tex.stackexchange.com/questions/297283/order-references-alphabetically-with-elsarticle-num-names-bst
% and https://github.com/erelsgl/erelsgl.github.io/blob/master/papers/elsarticle-num-names-alpha.bst
%
% To revert to the prior bibliographystyle, change the following line back to:
%             \bibliographystyle{elsarticle-num-names}

\bibliographystyle{elsarticle-num-names-alpha}
\bibliography{harmonia-bib}


\end{document}


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}
